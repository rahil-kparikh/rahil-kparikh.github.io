---
title: "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning"
collection: publications
permalink: /publication/model_inversion_nlu_models
excerpt: 'This work introduces a prompt-tuning method to control memorized content extraction in LLMs, demonstrating both attack and defense strategies. Using GPT-Neo models, they show their attack increases extraction rates by 9.3 percentage points while their defense reduces extraction by up to 97.7% with minimal impact on model utility.'
venue: 'Association for Computational Linguistics (ACL)'
paperurl: 'https://aclanthology.org/2023.acl-short.129/'
citation: 'Mustafa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, and Rahul Gupta. 2023. Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1512â€“1521, Toronto, Canada. Association for Computational Linguistics.'
---
[Download paper here](https://aclanthology.org/2023.acl-short.129.pdf)